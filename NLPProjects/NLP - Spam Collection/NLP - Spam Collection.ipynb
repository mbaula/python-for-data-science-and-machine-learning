{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Spam Collection\n",
    "\n",
    "- Compile Documents\n",
    "- Featurize them\n",
    "- Compare features\n",
    "\n",
    "Imagine Two Documents ('Blue House' and 'Red House')\n",
    "'Blue House' -> (red,blue,house) -> (0,1,1)\n",
    "'Red House' -> (red,blue,house) -> (1,0,1)\n",
    "A document represented as a vector of word counts is a 'bag of words.' You can use the cosine similarity on the vectors to determine similarity (dot product). Improve on Bag of Words by adjusting word counts based on their frequency in corpus (group of all the documents). We can use TF-IDF(Term Frequency - Inverse Document Frequency). \n",
    "\n",
    "Term Frequency : importance of the term within that document\n",
    "- TF(d,t) = # of occurences of term t in document d\n",
    "Inverse Document Frequency: importance of the term in the corpus\n",
    "- IDF(t) = log(D/t) where D = total # of documents, t = # of docs within the term\n",
    "\n",
    "Mathematically, TF-IDF is expressed as:\n",
    "**(W)x,y = (tf)x,y * log(N/(df)x)**\n",
    "\n",
    "(tf)x,y = frequence of x in y\n",
    "(df)x = # of docs containing x\n",
    "N = total # of docs\n",
    "\n",
    "For this project we will be using data from [UCI datasets](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download_shell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SMS Spam Collection Dataset classifies text as spam or ham (normal text message). We'll use this data to create a spam detection filter with python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [line.rstrip() for line in open('smsspamcollection/SMSSpamCollection')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam\\t07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection of texts is called a corpus. Let's print the first ten messages and number them using enumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "1 ham\tOk lar... Joking wif u oni...\n",
      "2 spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "3 ham\tU dun say so early hor... U c already then say...\n",
      "4 ham\tNah I don't think he goes to usf, he lives around here though\n",
      "5 spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv\n",
      "6 ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "7 ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "8 spam\tWINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "9 spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n"
     ]
    }
   ],
   "source": [
    "for mess_no, message in enumerate(messages[:10]):\n",
    "    print(mess_no,message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the spacing we can tell this is a TSV('tab seperated values') file, where the first column is a label saying whether the message is 'ham' or 'spam.' The second column is the message itself. (Our numbers are not parts of the file, they're from the enumerate call).\n",
    "\n",
    "Using these labeled ham and spam examples, we'll train a machine learning model to classify between ham and spam automatically. Then with this model we can classify arbitrary unlabeled messages as ham or spam.\n",
    "\n",
    "Let's use pandas to parse this TSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t',\n",
    "                      names=['label','message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "label      5572 non-null object\n",
      "message    5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n"
     ]
    }
   ],
   "source": [
    "messages.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use groupby to describe by label, this way we can begin to think about the feature that seperate ham and spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to start thinking about features we're going to be using. This is called features engineering which is a large part of spam direction. Let's make a new column to detect how long the text messages are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['length'] = messages['message'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x174800d4fd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "messages['length'].plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be a bimodal distribution. Perhaps text length is a good feature to think about. Let's try to explain why some messages go all the way past 800."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>80.489950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>59.942907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>122.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>910.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            length\n",
       "count  5572.000000\n",
       "mean     80.489950\n",
       "std      59.942907\n",
       "min       2.000000\n",
       "25%      36.000000\n",
       "50%      62.000000\n",
       "75%     122.000000\n",
       "max     910.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[messages['length']>=800]['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the outlier seems to be some sort of love message, and chicken curry. Interesting. Anyways, let's try to get back on track!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x000001748077EB00>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x000001748052CF28>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.hist(column='length',by='label',bins=75,figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Spam messages tend to be much more longer than ham messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Text Pre-Processing\n",
    " \n",
    "Main issue with the data is that it's all in text format. All the machine leaning algoriths like logistic regression, SVM, etc. needs some sort of numerical feature vector in order to perform the classification task. There are many methods to convert a corpus to a vector format. The simplest is the bag of words approach, where each unique word in a text is represented by one number.\n",
    " \n",
    "We'll convert raw messages (sequence of characters) into vectors (sequences of numbers). Let's first write a function that will split a message into individual words. We'll also remove common words such as ('the', 'a', etc) We will take advantage of the NLTK library. It's the standard library in python for processing text and has a lot of useful features. \n",
    "\n",
    "Let's create a function to process the string in the message column, then use apply() in pandas to process all the text into one DataFrame. We'll take advantage of Python's built-in string library to get a quick list of all the possible punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mess = 'Sample message! Notice: it has punctuation.'\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nopunc = [char for char in mess if char not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 'a',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'm',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'a',\n",
       " 'g',\n",
       " 'e',\n",
       " ' ',\n",
       " 'N',\n",
       " 'o',\n",
       " 't',\n",
       " 'i',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'i',\n",
       " 't',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'p',\n",
       " 'u',\n",
       " 'n',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are very common words that wouldn't be very helpful when we try to distinguish between spam and ham text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sample message Notice it has punctuation'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc = ''.join(nopunc)\n",
    "nopunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'it', 'has', 'punctuation']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mess = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'punctuation']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_mess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to make clean_mess we iterated, word for word in the list nopunc.split() as long as the lowercase version of that word is not in the common stopwords. Notice that it and has, was removed in the clean_mess list.\n",
    "\n",
    "Now let's put this entire process into a function that we can .apply() to the entire dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mess will be the initial string\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    1. remove punctuation\n",
    "    2. remove stopwords\n",
    "    3. return list of clean text words\n",
    "    \"\"\"\n",
    "    \n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3        [U, dun, say, early, hor, U, c, already, say]\n",
       "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['message'].head(5).apply(text_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing Normalization\n",
    "\n",
    "Note that therea are more ways to normalize this text such as [Stemming](https://en.wikipedia.org/wiki/Stemming) or distinguishing by [part of speech](http://www.nltk.org/book/ch05.html).\n",
    "\n",
    "NLTK has built-in tools on these methods, but sometimes don't work well for text-messages due to the way a lot of people tend to use abbreviations/shorthand, for example:\n",
    "    'Nah dawg, idk! wut time u headin to da club?'\n",
    "\n",
    "versus\n",
    "    'No dog, i don't know! What time you heading to the club?'\n",
    "    \n",
    "Some text normalization methods has trouble with this type of shorthand. For the purpose of this project we'll just focus on using what we have and convert our list of words to an actual vector that scikit-learn can use.\n",
    "\n",
    "# Vectorization\n",
    "\n",
    "Right now, we have messages as tokens and we need to convert each of these messages into a vector that scikit learn's algorith models can work with. We'll convert each message, represented as a list of tokens, into a vector that machine learning can understand.\n",
    "\n",
    "We'll do that in three steps using the bag of words model:\n",
    "1. Count how many times a word occurs in each message (term frequency)\n",
    "2. Weigh the counts, so frequent tokens get lower weight (inverse document frequency)\n",
    "3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n",
    "\n",
    "Let's begin with the first step:\n",
    "\n",
    "Each vector will have as many dimensions as there are unique words in the SMS corpus. We'll use scikitlearn's **CountVectorizer**. This model will convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "We can imagine this as a 2-dimensional matric where the 1-dimension is the entire vocabulary (1 row per word) and the other dimesion are actual documents (1 column per text message).\n",
    "\n",
    "<table border = “1“>\n",
    "<tr>\n",
    "<th></th> <th>Message 1</th> <th>Message 2</th> <th>...</th> <th>Message N</th> \n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word 1 Count</b></td><td>0</td><td>1</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word 2 Count</b></td><td>0</td><td>0</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>...</b></td> <td>1</td><td>2</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word N Count</b></td> <td>0</td><td>1</td><td>...</td><td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Since there are many messages, there are a lot of zero counts for the presence of that word in the document. SciKit Learn will output a [Sparse Matrix](https://en.wikipedia.org/wiki/Sparse_matrix) because of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11425\n"
     ]
    }
   ],
   "source": [
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess4 = messages['message'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U dun say so early hor... U c already then say...\n"
     ]
    }
   ],
   "source": [
    "print(mess4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow4 = bow_transformer.transform([mess4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4068)\t2\n",
      "  (0, 4629)\t1\n",
      "  (0, 5261)\t1\n",
      "  (0, 6204)\t1\n",
      "  (0, 6222)\t1\n",
      "  (0, 7186)\t1\n",
      "  (0, 9554)\t2\n"
     ]
    }
   ],
   "source": [
    "print(bow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11425)\n"
     ]
    }
   ],
   "source": [
    "print(bow4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically there are 7 unique words in message 4 and 2 of them appear twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_transformer.get_feature_names()[4068]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_transformer.get_feature_names()[9554]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use .transform on our bag of words transformed object and transfore the entire DataFrame of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_bow = bow_transformer.transform(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (5572, 11425)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Sparse Matrix: ', messages_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50548"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_bow.nnz #non zero appearances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After counting, the term weighting and normalization can be done with TF-IDF, using scikit-learn's TfidTransformer.\n",
    "\n",
    "# TF-IDF\n",
    "TF_IDF stands for term frequency-inverse document frequency, and the tf-idf weight is used in information retrieval and text mining. The weight measures how important a word is to a document in a collection of corpus. The importance increases proportionally to the number of times a wod appears in the document but is offset by the frequency of word in the corpus. Variations of the tf-idf scheme is used by search engines as central tool in scoring and ranking a document's relevance given a user query.\n",
    "\n",
    "One of the simplest ranking functions is computed by summing the tf-idf for each query term, and the more sophisticated ranking functions are variations of this model.\n",
    "\n",
    "The tf-idf weight is composed by two terms. The first computes the normalized Term Frequency (TF) which is the number of times a word appears in the document, divided by the number of words. The second term is inverse document frequency (IDF) which is the logarithm of the number of documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "**TF: Term Frequency** Since every document is different in length, it's possible a term would appear much more times in long documents than in shorter ones. So, the term frequency is divided by the document length (# of terms in the document) as a way of normalization.\n",
    "\n",
    "*TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).*\n",
    "\n",
    "**IDF: Inverse Document Frequency** measures how important a term is. When you compute TF, all terms are considered equally important, however in certain terms such as 'is', 'of', etc, they appear a lot but is of little importance. Thus we need to weigh down the frequent terms while scaling up the rare ones.\n",
    "\n",
    "*IDF(t) = log_e(Total number of documents / Number of documents with term t in it).*\n",
    "\n",
    "**EXAMPLE:** Consider a document that has 100 words where cat appears 3 times.\n",
    "\n",
    "The tf for cat is (3/100) = 0.03. Now if there are 10 million documents and the word cat appears in one thousand of these, then the inverse document is calculated to be log(10 000 000 / 1000) = 4. Thus the tf-idf wieght is the product of these quantities: 0.03*4 = 0.12\n",
    "\n",
    "Let's implement this with scikit learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf4 = tfidf_transformer.transform(bow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9554)\t0.5385626262927564\n",
      "  (0, 7186)\t0.4389365653379857\n",
      "  (0, 6222)\t0.3187216892949149\n",
      "  (0, 6204)\t0.29953799723697416\n",
      "  (0, 5261)\t0.29729957405868723\n",
      "  (0, 4629)\t0.26619801906087187\n",
      "  (0, 4068)\t0.40832589933384067\n"
     ]
    }
   ],
   "source": [
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the idf of a particular word, like club and beer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.32310369457549\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['club']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.93254160700959\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['beer']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_tfidf = tfidf_transformer.transform(messages_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways that data can be preprocessed and vectorized. These steps involves feature engineering and building a 'pipeline.'\n",
    "\n",
    "# Training a model\n",
    "\n",
    "Now that messages are represented as vectors, we can finally train our spam/ham classifier. We can use almost any classification algorithm but we'll choose to use the Naive Bayes classifier algortihm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detect_model = MultinomialNB().fit(messages_tfidf,messages['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_detect_model.predict(tfidf4)[0] #detects tfidf4 as ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['label'][3] #it actually is ham so we're predicting correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = spam_detect_model.predict(messages_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'spam', ..., 'ham', 'ham', 'ham'], dtype='<U4')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      4825\n",
      "        spam       1.00      0.85      0.92       747\n",
      "\n",
      "    accuracy                           0.98      5572\n",
      "   macro avg       0.99      0.92      0.95      5572\n",
      "weighted avg       0.98      0.98      0.98      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(messages['label'],all_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with the current model we accurately detected whether a text message would be spam or ham with up to 98% accuracy! However we evaluated accuracy on the same dataset we used for training, and **you should never evaluate on the same dataset you train on**\n",
    "\n",
    "This evaluation tells us nothing about the true predictive power of our model. We should split the data into training/test set, where the model only ever sees training data during its model fitting and parameter tuning. The test data is never used. This would be used our final evaluation to represent the true predictive power performance\n",
    "\n",
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_train,msg_test,label_train,label_test = train_test_split(messages['message'],messages['label'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000                           Hmph. Go head, big baller.\n",
       "3830             Sure, I'll see if I can come by in a bit\n",
       "902           How is it possible to teach you. And where.\n",
       "1285                         But if she.s drinkin i'm ok.\n",
       "5112    December only! Had your mobile 11mths+? You ar...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pipeline\n",
    "Let's run our model using pipeline to store the pipeline of the workflow. This allows us to set up the transformations that we'll do to data for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=text_process)), #strings to token integer counts\n",
    "    ('tfidf',TfidfTransformer()), #integer counts to tfidf scores\n",
    "    ('classifier', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x000001748AA3E378>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      1462\n",
      "        spam       1.00      0.71      0.83       210\n",
      "\n",
      "    accuracy                           0.96      1672\n",
      "   macro avg       0.98      0.85      0.90      1672\n",
      "weighted avg       0.96      0.96      0.96      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(label_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a classification report for our true testing set. We've accurately predicted spam vs ham up to 96% !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
